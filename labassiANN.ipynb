{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM/9PkWuliYg8wqbOhjHn7h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ekagra444/simpleANN/blob/main/labassiANN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANN classification"
      ],
      "metadata": {
        "id": "4zv1KSKp5SMQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load and prepare the dataset\n",
        "def load_data():\n",
        "    data = load_breast_cancer()\n",
        "    X = data.data\n",
        "    y = data.target.reshape(-1, 1)  # Reshape to (n_samples, 1)\n",
        "\n",
        "    # Split into train and test sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Standardize features\n",
        "    scaler = StandardScaler()\n",
        "    X_train = scaler.fit_transform(X_train)\n",
        "    X_test = scaler.transform(X_test)\n",
        "\n",
        "    return X_train, X_test, y_train, y_test"
      ],
      "metadata": {
        "id": "gsBrmssF5lMq"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "collapsed": true,
        "id": "aLn3f6uTy-Xn"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Activation functions and their derivatives\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return sigmoid(x) * (1 - sigmoid(x))\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_derivative(x):\n",
        "    return (x > 0).astype(float)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Neural Network class\n",
        "class SimpleNeuralNetwork:\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        # Initialize weights\n",
        "        self.W1 = np.random.randn(input_size, hidden_size) * np.sqrt(2/input_size)\n",
        "      #  self.W1 = np.random.randn(input_size, hidden_size) * 0.01 not good with relu activation\n",
        "        self.b1 = np.zeros((1, hidden_size))\n",
        "        self.W2 = np.random.randn(hidden_size, output_size) * np.sqrt(2/hidden_size)\n",
        "        self.b2 = np.zeros((1, output_size))\n",
        "\n",
        "    def forward(self, X):\n",
        "        # Hidden layer\n",
        "        self.z1 = np.dot(X, self.W1) + self.b1\n",
        "        self.a1 = relu(self.z1)\n",
        "\n",
        "        # Output layer\n",
        "        self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
        "        self.a2 = sigmoid(self.z2)\n",
        "\n",
        "        return self.a2\n",
        "\n",
        "    def backward(self, X, y, learning_rate):\n",
        "        m = X.shape[0]  # Number of samples\n",
        "\n",
        "        # Output layer error\n",
        "        dZ2 = self.a2 - y\n",
        "        dW2 = (1/m) * np.dot(self.a1.T, dZ2)\n",
        "        db2 = (1/m) * np.sum(dZ2, axis=0, keepdims=True)\n",
        "\n",
        "        # Hidden layer error\n",
        "        dZ1 = np.dot(dZ2, self.W2.T) * relu_derivative(self.z1)\n",
        "        dW1 = (1/m) * np.dot(X.T, dZ1)\n",
        "        db1 = (1/m) * np.sum(dZ1, axis=0, keepdims=True)\n",
        "\n",
        "        # Update weights\n",
        "        self.W2 -= learning_rate * dW2\n",
        "        self.b2 -= learning_rate * db2\n",
        "        self.W1 -= learning_rate * dW1\n",
        "        self.b1 -= learning_rate * db1\n",
        "\n",
        "    def train(self, X, y, epochs, learning_rate):\n",
        "        for epoch in range(epochs):\n",
        "            # Forward pass\n",
        "            output = self.forward(X)\n",
        "\n",
        "            # Backward pass and weight update\n",
        "            self.backward(X, y, learning_rate)\n",
        "\n",
        "            # Calculate loss (binary cross-entropy)\n",
        "            loss = -np.mean(y * np.log(output) + (1 - y) * np.log(1 - output))\n",
        "\n",
        "            # Print progress\n",
        "            if epoch % 5 == 0:\n",
        "                predictions = (output > 0.5).astype(int)\n",
        "                accuracy = accuracy_score(y, predictions)\n",
        "                print(f\"Epoch {epoch}, Loss: {loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "2RcsNCFA5snY"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Load and prepare data\n",
        "    X_train, X_test, y_train, y_test = load_data()\n",
        "\n",
        "    # Network parameters\n",
        "    input_size = X_train.shape[1]\n",
        "    hidden_size = 8\n",
        "    output_size = 1\n",
        "\n",
        "    # Create and train network\n",
        "    nn = SimpleNeuralNetwork(input_size, hidden_size, output_size)\n",
        "    nn.train(X_train, y_train, epochs=20, learning_rate=0.7)\n",
        "    # Test the network\n",
        "    test_output = nn.forward(X_test)\n",
        "    test_predictions = (test_output > 0.5).astype(int)\n",
        "    test_accuracy = accuracy_score(y_test, test_predictions)\n",
        "    print(f\"\\nTest Accuracy: {test_accuracy:.4f}\")\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sf9lNu_l5xHO",
        "outputId": "9431b35f-ec5d-4e6e-f442-f336158801c3"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.6620, Accuracy: 0.4747\n",
            "Epoch 5, Loss: 0.2073, Accuracy: 0.9407\n",
            "Epoch 10, Loss: 0.1228, Accuracy: 0.9626\n",
            "Epoch 15, Loss: 0.0947, Accuracy: 0.9736\n",
            "\n",
            "Test Accuracy: 0.9825\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reason for choosing the dataset -\n",
        "\n",
        "Clean medical data with 30 tumor features\n",
        "Perfect 62%-38% class balance (benign/malignant)\n",
        "\n",
        "Small (569 samples) but enough for meaningful training\n",
        "\n",
        "Proven ANN performance (>95% accuracy achievable)\n",
        "\n",
        "Direct clinical relevance for cancer diagnosis\n",
        "\n",
        "The results empirically validate the importance of proper weight initialization in neural networks. He initialization's mathematical formulation specifically addresses challenges posed by ReLU activations, enabling:\n",
        "\n",
        "\n",
        "*   Effective gradient flow\n",
        "*   Rapid convergence\n",
        "*   Superior generalization performance\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-TCsxuIJ-Onn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANN regression"
      ],
      "metadata": {
        "id": "kVPHVblR-j8Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "class RegressionNN:\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        # He initialization for ReLU\n",
        "        self.W1 = np.random.randn(input_size, hidden_size) * np.sqrt(2/input_size)\n",
        "        self.b1 = np.zeros((1, hidden_size))\n",
        "\n",
        "        # Xavier initialization for linear output\n",
        "        self.W2 = np.random.randn(hidden_size, output_size) * np.sqrt(1/hidden_size)\n",
        "        self.b2 = np.zeros((1, output_size))\n",
        "\n",
        "    def relu(self, x):\n",
        "        return np.maximum(0, x)\n",
        "\n",
        "    def relu_derivative(self, x):\n",
        "        return (x > 0).astype(float)\n",
        "\n",
        "    def forward(self, X):\n",
        "        # Hidden layer\n",
        "        self.z1 = np.dot(X, self.W1) + self.b1\n",
        "        self.a1 = self.relu(self.z1)\n",
        "\n",
        "        # Output layer (linear activation for regression)\n",
        "        self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
        "        return self.z2\n",
        "    def backward(self, X, y, y_pred, learning_rate):\n",
        "      m = X.shape[0]  # Number of samples\n",
        "\n",
        "      # MSE derivative (factor of 2 is often omitted by absorbing into learning rate)\n",
        "      dZ2 = (y_pred - y) / m  # This is ∂L/∂z2 where L = MSE\n",
        "\n",
        "      # Rest of backpropagation remains the same\n",
        "      dW2 = np.dot(self.a1.T, dZ2)\n",
        "      db2 = np.sum(dZ2, axis=0, keepdims=True)\n",
        "\n",
        "      dZ1 = np.dot(dZ2, self.W2.T) * self.relu_derivative(self.z1)\n",
        "      dW1 = np.dot(X.T, dZ1)\n",
        "      db1 = np.sum(dZ1, axis=0, keepdims=True)\n",
        "\n",
        "      # Update parameters\n",
        "      self.W2 -= learning_rate * dW2\n",
        "      self.b2 -= learning_rate * db2\n",
        "      self.W1 -= learning_rate * dW1\n",
        "      self.b1 -= learning_rate * db1\n",
        "    def train(self, X, y, epochs, learning_rate):\n",
        "        for epoch in range(epochs):\n",
        "            # Forward pass\n",
        "            y_pred = self.forward(X)\n",
        "\n",
        "            # Compute loss (MSE)\n",
        "            loss = mean_squared_error(y, y_pred)\n",
        "\n",
        "            # Backward pass\n",
        "            self.backward(X, y, y_pred, learning_rate)\n",
        "\n",
        "            # Print training progress\n",
        "            if epoch % 5 == 0:\n",
        "                r2 = r2_score(y, y_pred)\n",
        "                print(f\"Epoch {epoch}: MSE = {loss:.4f}, R2 = {r2:.4f}\")\n",
        "\n",
        "# Load and prepare California housing dataset\n",
        "def load_data():\n",
        "    data = fetch_california_housing()\n",
        "    X = data.data\n",
        "    y = data.target.reshape(-1, 1)\n",
        "\n",
        "    # Split and scale data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    scaler = StandardScaler()\n",
        "    X_train = scaler.fit_transform(X_train)\n",
        "    X_test = scaler.transform(X_test)\n",
        "\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Load data\n",
        "    X_train, X_test, y_train, y_test = load_data()\n",
        "\n",
        "    # Network parameters\n",
        "    input_size = X_train.shape[1]\n",
        "    hidden_size = 10\n",
        "    output_size = 1\n",
        "\n",
        "    # Create and train network\n",
        "    nn = RegressionNN(input_size, hidden_size, output_size)\n",
        "    nn.train(X_train, y_train, epochs=20, learning_rate=0.5)\n",
        "\n",
        "    # Evaluate on test set\n",
        "    test_pred = nn.forward(X_test)\n",
        "    test_mse = mean_squared_error(y_test, test_pred)\n",
        "    test_r2 = r2_score(y_test, test_pred)\n",
        "\n",
        "    print(f\"\\nFinal Test Results: MSE = {test_mse:.4f}, R2 = {test_r2:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "we5mVKik-mIC",
        "outputId": "4b491324-c3cf-4892-b4a6-d60f049384cc"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: MSE = 3.1099, R2 = -1.3264\n",
            "Epoch 5: MSE = 1.4028, R2 = -0.0494\n",
            "Epoch 10: MSE = 0.6885, R2 = 0.4849\n",
            "Epoch 15: MSE = 0.6056, R2 = 0.5470\n",
            "\n",
            "Final Test Results: MSE = 0.5846, R2 = 0.5539\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reason for choosing the dataset -\n",
        "\n",
        "Classic 8-feature housing price prediction problem\n",
        "\n",
        "20,640 samples - large enough for regression\n",
        "\n",
        "Mixed-scale features (tests scaling implementation)\n",
        "\n",
        "Continuous output tests MSE/L2 loss handling\n",
        "\n",
        "Real-world economic significance\n",
        "\n",
        "Epoch 0: MSE = 3.1099, R2 = -1.3264\n",
        "Epoch 5: MSE = 1.4028, R2 = -0.0494\n",
        "Epoch 10: MSE = 0.6885, R2 = 0.4849\n",
        "Epoch 15: MSE = 0.6056, R2 = 0.5470\n",
        "\n",
        "\n",
        "Final Test Results: MSE = 0.5846, R2 = 0.5539\n",
        "\n",
        "i had to choose higher vvalue of learning rate because we just had one hiden layer and he is used to  ininitiate because of relu activation function"
      ],
      "metadata": {
        "id": "cLAPkBe7GQxx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparison of output for ANN vs KNN-\n"
      ],
      "metadata": {
        "id": "Pun4dsIPJ8UD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For Regression-\n",
        "\n",
        "KNN's local averaging works well for this dataset's patterns\n",
        "\n",
        "ANN is too simple (underfitting) or need tuning\n",
        "\n",
        "Dataset size is small for ANN to learn effectively and moreover we don't have many hidden layers(only 1) which caused KNN to outperform"
      ],
      "metadata": {
        "id": "82ijZ6bSKb8C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For Classification-\n",
        "\n",
        "KNN:\n",
        "\n",
        "Accuracy: 1.00 (Perfect classification)\n",
        "\n",
        "No training needed.\n",
        "\n",
        "ANN:\n",
        "\n",
        "Test Accuracy: 98.25%\n",
        "\n",
        "Trained over 15 epochs (Loss ↓ 0.66 → 0.09).\n",
        "\n",
        "Key Insight\n",
        "KNN performs better on Iris due to its simplicity and the dataset’s linear separability.\n",
        "\n",
        "ANN achieves near-perfect results but requires training and slightly underperforms.\n",
        "\n",
        "Conclusion: For small, simple datasets like Iris, KNN is more efficient. For complex data, ANN generalizes better."
      ],
      "metadata": {
        "id": "GutzGCjQKx9u"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jVJfHxTiGRbG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}